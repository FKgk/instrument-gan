{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras import models, layers, losses, optimizers\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.io import wavfile\nimport os","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/'\nsec = 4\ndata = [[], []]\nfolder = os.listdir(path)\n\nfor piano, syth in zip(os.listdir(path + folder[0]),  os.listdir(path+folder[1])):\n    fn = [piano, syth]\n    for i in range(2):\n        samplerate, file = wavfile.read(path + folder[i] +'/' + fn[i])\n        f = np.array(file, dtype=float)\n        for j in range(0, f.shape[0] - sec * samplerate, sec * samplerate):\n            data[i].append(f[j:j + sec * samplerate, :])","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train = np.array(data)\n# piano, synth = np.array(data)\n# x_train, x_test, y_train, y_test = train_test_split(piano, synth, test_size=0.4)\n\ninput_size = (x_train.shape[1], x_train.shape[2])\nprint(x_train.shape)","execution_count":4,"outputs":[{"output_type":"stream","text":"(166, 176400, 2)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Gan 모델 생성"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Generator(lr=1e-3):\n    inputs = layers.Input(shape=input_size)\n\n    # down sampling\n    out = layers.Conv1D(filters=128, kernel_size=2, strides=1, padding='same', activation='relu')(inputs)\n    out = layers.Conv1D(filters=256, kernel_size=2, strides=2, padding='same', activation='relu')(out)\n    out = layers.UpSampling1D()(out)\n    out = layers.Conv1D(filters=512, kernel_size=2, strides=4, padding='same', activation='relu')(out)\n    out = layers.UpSampling1D()(out)\n    out = layers.Conv1D(filters=1024, kernel_size=2, strides=8, padding='same', activation='relu')(out)\n    out = layers.UpSampling1D()(out)\n    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=128, kernel_size=1, strides=1, padding='same', activation='relu')(out)\n    print(out.shape)\n\n    # out = layers.Reshape((out.shape[-1], out.shape[1]))(out)\n    # up sampling\n    out = layers.UpSampling1D()(out)\n    print(out.shape)\n    out = layers.Conv1D(filters=2, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n    print(out.shape)\n    out = layers.UpSampling1D(sec * samplerate)(out)\n    print(out.shape)\n\n#     out = layers.Dense(input_size[0], activation='relu')(out)\n#     out = layers.Reshape(input_size)(out)\n\n    model = models.Model(inputs, out)\n    model.compile(optimizer=optimizers.Adam(lr), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n    \n    return model\nGenerator()","execution_count":null,"outputs":[{"output_type":"stream","text":"(None, 22050, 128)\n(None, 44100, 128)\n(None, 44100, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator(lr=1e-3):\n    inputs = layers.Input(shape=input_size)\n    \n    out = layers.Conv1D(filters=128, kernel_size=2, strides=1, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=512, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=1024, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=1024, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n    out = layers.Conv1D(filters=64, kernel_size=1, strides=1, padding='same', activation='relu')(out)\n\n    out = layers.Flatten()(out)\n    out = layers.Dense(1, activation='sigmoid')(out)\n    \n    model = models.Model(inputs, out)\n    model.compile(optimizer=optimizers.Adam(lr), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Gan(discriminator, generator, lr=1e-3):\n    discriminator.trainable=False\n    \n    inputs = layers.Input(shape=input_size)\n    x = generator(inputs)\n    out = discriminator(x)\n    \n    gan = models.Model(inputs, out)\n    gan.compile(optimizer=optimizers.Adam(lr=lr), loss=losses.binary_crossentropy)\n    return gan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 하이퍼 파라미터 설정"},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = Generator(lr=1e-5)\ndiscriminator = discriminator(lr=1e-5)\ngan = Gan(discriminator, generator, lr=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=10\nbatch_size = 5\nbatch_count = int(x_train.shape[0] / 10)\ngan_losses = list()\n\nprint(f\"batch_count: {batch_count}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 모델 학습"},{"metadata":{"trusted":true},"cell_type":"code","source":"for e in range(1,epochs+1):\n    batch_loss = 0\n    for index, i in enumerate(range(0, x_train.shape[0] - batch_size, batch_size)):\n        x = x_train[i:i+batch_size, :, :]\n        y = y_train[i:i+batch_size, :, :]\n\n        # Generate fake MNIST images from noised input\n        generated_synth = generator.predict(x)\n        synth_batch =y[np.random.randint(low=0,high=x.shape[0],size=batch_size), :, :]\n        X = np.concatenate([synth_batch, generated_synth])\n\n        y_dis=np.zeros(2*batch_size)\n        y_dis[:batch_size]=0.9\n\n        discriminator.trainable=True\n        discriminator.train_on_batch(X, y_dis)\n\n        y_gen = np.ones(batch_size)\n\n        discriminator.trainable=False\n        loss = gan.train_on_batch(x, y_gen)\n        \n        batch_loss += loss\n        \n        if index % 15 == 0:\n            print(f\"Epoch {e}/{epochs} Batch:{index} loss:{loss}\")\n\n    batch_loss /= batch_count\n    print(f\"Epoch {e}/{epochs} loss:{batch_loss}\")\n    gan_losses.append(batch_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 학습 시각화"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"train loss\")\nplt.plot(list(range(len(gan_losses))), gan_losses)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 테스트 파일 생성"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_count = 5\n\nsample = np.random.randint(0, x_train.shape[0] - sample_count)\ntest = x_train[sample:sample + sample_count, : , :]\n\ngenerated_synth = generator.predict(test)\n\nfor i in range(sample_count):\n    wavfile.write('test'+str(i)+'.wav', samplerate, generated_synth[i])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}