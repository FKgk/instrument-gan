{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import wavfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "path = '/kaggle/input/'\n",
    "sec = 4\n",
    "data = [[], []]\n",
    "folder = os.listdir(path)\n",
    "\n",
    "for piano, syth in zip(os.listdir(path + folder[0]),  os.listdir(path+folder[1])):\n",
    "    fn = [piano, syth]\n",
    "    for i in range(2):\n",
    "        samplerate, file = wavfile.read(path + folder[i] +'/' + fn[i])\n",
    "        f = np.array(file, dtype=float)\n",
    "        for j in range(0, f.shape[0] - sec * samplerate, sec * samplerate):\n",
    "            data[i].append(f[j:j + sec * samplerate, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.array(data)\n",
    "# piano, synth = np.array(data)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(piano, synth, test_size=0.4)\n",
    "\n",
    "input_size = (x_train.shape[1], x_train.shape[2])\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(lr=1e-3):\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "\n",
    "    # down sampling\n",
    "    out = layers.Conv1D(filters=128, kernel_size=15, strides=1, padding='same', activation='relu')(inputs)\n",
    "    out = layers.Conv1D(filters=256, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "\n",
    "    # repeat 4\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    \n",
    "    # up sampling\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=2, kernel_size=15, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "\n",
    "    model = models.Model(inputs, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n",
    "        \n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(lr=1e-3):\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    \n",
    "    out = layers.Conv1D(filters=128, kernel_size=2, strides=1, padding='same', activation='relu')(inputs)\n",
    "    out = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=64, kernel_size=1, strides=1, padding='same', activation='relu')(out)\n",
    "\n",
    "    out = layers.Flatten()(out)\n",
    "    out = layers.Dense(1024, activation='relu')(out)\n",
    "    out = layers.Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    model = models.Model(inputs, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n",
    "    \n",
    "    return model\n",
    "discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gan(discriminator, generator, lr=1e-3):\n",
    "    discriminator.trainable=False\n",
    "    \n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    x = generator(inputs)\n",
    "    out = discriminator(x)\n",
    "    \n",
    "    gan = models.Model(inputs, out)\n",
    "    gan.compile(optimizer=optimizers.Adam(lr=lr), loss=losses.binary_crossentropy)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(lr=1e-5)\n",
    "discriminator = discriminator(lr=1e-5)\n",
    "gan = Gan(discriminator, generator, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size = 5\n",
    "batch_count = int(x_train.shape[0] / 10)\n",
    "gan_losses = list()\n",
    "\n",
    "print(f\"batch_count: {batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1,epochs+1):\n",
    "    batch_loss = 0\n",
    "    for index, i in enumerate(range(0, x_train.shape[0] - batch_size, batch_size)):\n",
    "        x = x_train[i:i+batch_size, :, :]\n",
    "        y = y_train[i:i+batch_size, :, :]\n",
    "\n",
    "        # Generate fake MNIST images from noised input\n",
    "        generated_synth = generator.predict(x)\n",
    "        synth_batch =y[np.random.randint(low=0,high=x.shape[0],size=batch_size), :, :]\n",
    "        X = np.concatenate([synth_batch, generated_synth])\n",
    "\n",
    "        y_dis=np.zeros(2*batch_size)\n",
    "        y_dis[:batch_size]=0.9\n",
    "\n",
    "        discriminator.trainable=True\n",
    "        discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        y_gen = np.ones(batch_size)\n",
    "\n",
    "        discriminator.trainable=False\n",
    "        loss = gan.train_on_batch(x, y_gen)\n",
    "        \n",
    "        batch_loss += loss\n",
    "        \n",
    "        if index % 15 == 0:\n",
    "            print(f\"Epoch {e}/{epochs} Batch:{index} loss:{loss}\")\n",
    "\n",
    "    batch_loss /= batch_count\n",
    "    print(f\"Epoch {e}/{epochs} loss:{batch_loss}\")\n",
    "    gan_losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"train loss\")\n",
    "plt.plot(list(range(len(gan_losses))), gan_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 5\n",
    "\n",
    "sample = np.random.randint(0, x_train.shape[0] - sample_count)\n",
    "test = x_train[sample:sample + sample_count, : , :]\n",
    "\n",
    "generated_synth = generator.predict(test)\n",
    "\n",
    "for i in range(sample_count):\n",
    "    wavfile.write('test'+str(i)+'.wav', samplerate, generated_synth[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
