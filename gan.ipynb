{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'process_data'\n",
    "x_folder = 'marimba'\n",
    "y_folder = 'synthesizer'\n",
    "\n",
    "sec = 4\n",
    "samplerate = 44100\n",
    "time_length = sec * samplerate\n",
    "input_size = (time_length, 2)\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 5\n",
    "sample_count = 3\n",
    "g_lr, d_lr, gan_lr = [1e-4, 1e-4, 1e-4]\n",
    "\n",
    "sample_save_path = 'test'\n",
    "model_save_path = 'model'\n",
    "visual_loss_save_path = \"visual_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(sample_save_path):\n",
    "    os.makedirs(sample_save_path)\n",
    "\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "if not os.path.isdir(visual_loss_save_path):\n",
    "    os.makedirs(visual_loss_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file(index):\n",
    "    return os.path.isfile(os.path.join(path, x_folder, str(index) + '.wav'))\n",
    "\n",
    "def read_data(index):\n",
    "    x = np.load(os.path.join(path, x_folder, str(index) + '.npy'))\n",
    "    y = np.load(os.path.join(path, y_folder, str(index) + '.npy'))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def count_data():\n",
    "    return min(len(os.listdir(os.path.join(path, x_folder))), len(os.listdir(os.path.join(path, y_folder))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = read_data(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(lr=1e-3):\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "\n",
    "    # down sampling\n",
    "    out = layers.Conv1D(filters=128, kernel_size=15, strides=1, padding='same', activation='relu')(inputs)\n",
    "    out = layers.Conv1D(filters=256, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "\n",
    "    # repeat 4\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    \n",
    "    # up sampling\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=2, kernel_size=15, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "\n",
    "    model = models.Model(inputs, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(lr=1e-3):\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    \n",
    "    out = layers.Conv1D(filters=128, kernel_size=2, strides=1, padding='same', activation='relu')(inputs)\n",
    "    out = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=512, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=3, strides=2, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=1024, kernel_size=5, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu')(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Conv1D(filters=64, kernel_size=1, strides=1, padding='same', activation='relu')(out)\n",
    "\n",
    "    out = layers.Flatten()(out)\n",
    "    out = layers.Dense(1024, activation='relu')(out)\n",
    "    out = layers.Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    model = models.Model(inputs, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gan(discriminator, generator, lr=1e-3):\n",
    "    discriminator.trainable=False\n",
    "    \n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    x = generator(inputs)\n",
    "    out = discriminator(x)\n",
    "    \n",
    "    gan = models.Model(inputs, out)\n",
    "    gan.compile(optimizer=optimizers.Adam(lr=lr), loss=losses.binary_crossentropy)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(lr=1e-5)\n",
    "discriminator = discriminator(lr=1e-5)\n",
    "gan = Gan(discriminator, generator, lr=1e-5)\n",
    "\n",
    "gan_losses = list()\n",
    "for e in range(1, epochs + 1):\n",
    "    batch_loss = 0\n",
    "\n",
    "    for index in range(count_data()):\n",
    "        x, y = read_data(index)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        generated_synth = generator.predict(x)\n",
    "        synth_batch =y[np.random.randint(low=0,high=x.shape[0],size=batch_size), :, :]\n",
    "        X = np.concatenate([synth_batch, generated_synth])\n",
    "\n",
    "        y_dis=np.zeros(2 * batch_size)\n",
    "        y_dis[:batch_size] = 0.9\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        y_gen = np.ones(batch_size)\n",
    "\n",
    "        discriminator.trainable = False\n",
    "        loss = gan.train_on_batch(x, y_gen)\n",
    "\n",
    "        batch_loss += loss\n",
    "        if index % 20 == 0:\n",
    "            print(f\"Batch:{index} loss:{loss}\")\n",
    "\n",
    "    batch_loss /= batch_count\n",
    "    print(f\"Epoch {e}/{epochs} loss:{batch_loss}\")\n",
    "    gan_losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train loss 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"train loss per epoch\")\n",
    "plt.plot(list(range(len(gan_losses))), gan_losses)\n",
    "plt.savefig(os.path.join(visual_loss_save_path, str(i) + \".png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변환한 sample 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(range(count_data()), 1, replace=False, p=None)\n",
    "test, _ = read_data(sample)\n",
    "\n",
    "generated_synth = generator.predict(test)\n",
    "\n",
    "for number in sample:\n",
    "    wavfile.write(os.path.join(sample_save_path, str(i) + \"-\" + str(number) + \".wav\"), samplerate, generated_synth[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(os.path.join(model_save_path, 'generator.h5'))\n",
    "discriminator.save(os.path.join(model_save_path, 'discriminator.h5'))\n",
    "gan.save(os.path.join(model_save_path, 'gan.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
