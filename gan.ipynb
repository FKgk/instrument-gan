{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import wavfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 읽기 및 sec초 단위로 자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "files = [[], []]\n",
    "folder = os.listdir(path)\n",
    "\n",
    "for piano, syth in zip(os.listdir(path + folder[0]),  os.listdir(path+folder[1])):\n",
    "    fn = [piano, syth]\n",
    "    for i in range(2):\n",
    "        samplerate, file = wavfile.read(path + folder[i] +'/' + fn[i])\n",
    "        files[i].append(np.array(file, dtype=float).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[], []]\n",
    "sec = 4 * samplerate\n",
    "\n",
    "for k in range(2):\n",
    "    for f in files[k]:\n",
    "        for i in range(0, f.shape[-1] - sec, sec):\n",
    "            data[k].append(f[:, i:i + sec])\n",
    "\n",
    "piano, synth = np.array(data)\n",
    "input_size = (piano.shape[1], piano.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    \n",
    "    out = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    \n",
    "    out = layers.Dense(input_size[-1], activation='relu')(out)\n",
    "\n",
    "    model = models.Model(inputs, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    \n",
    "    out = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n",
    "    \n",
    "    out = layers.Flatten()(out)\n",
    "    out = layers.Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    model = models.Model(inputs, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3), loss=losses.binary_crossentropy, metrics=['binary_crossentropy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gan(discriminator, generator):\n",
    "    discriminator.trainable=False\n",
    "    \n",
    "    inputs = layers.Input(shape=input_size)\n",
    "    x = generator(inputs)\n",
    "    out = discriminator(x)\n",
    "    \n",
    "    gan = models.Model(inputs, out)\n",
    "    gan.compile(optimizer=optimizers.Adam(lr=1e-3), loss=losses.binary_crossentropy)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = piano, synth\n",
    "# x_train, x_test, y_train, y_test = train_test_split(piano, synth, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size = 20\n",
    "batch_count = x_train.shape[0] / batch_size\n",
    "\n",
    "# Creating GAN\n",
    "generator = Generator()\n",
    "discriminator = discriminator()\n",
    "gan = Gan(discriminator, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 2\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 3\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 4\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 5\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 6\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 7\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 8\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 9\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "Epoch 10\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n",
      "(40, 2, 176400)\n",
      "(40,)\n"
     ]
    }
   ],
   "source": [
    "for e in range(1,epochs+1):\n",
    "    print(\"Epoch %d\" %e)\n",
    "    for i in range(0, x_train.shape[0] - batch_size, batch_size):\n",
    "        # Generate random noise as an input to initialize the generator\n",
    "        # noise = np.random.normal(0,1, [batch_size, 100])\n",
    "        x = x_train[i:i+batch_size, :, :]\n",
    "        y = y_train[i:i+batch_size, :, :]\n",
    "\n",
    "        # Generate fake MNIST images from noised input\n",
    "        generated_synth = generator.predict(x)\n",
    "\n",
    "        # Get a random set of  real images\n",
    "        synth_batch =y[np.random.randint(low=0,high=x.shape[0],size=batch_size), :, :]\n",
    "\n",
    "        # Construct different batches of real and fake data \n",
    "        X = np.concatenate([synth_batch, generated_synth])\n",
    "\n",
    "        # Labels for generated and real data\n",
    "        y_dis=np.zeros(2*batch_size)\n",
    "        y_dis[:batch_size]=1\n",
    "\n",
    "        # Pretrain discriminator on  fake and real data before starting the gan. \n",
    "        discriminator.trainable=True\n",
    "        \n",
    "        discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        # Tricking the noised input of the Generator as real data\n",
    "        y_gen = np.ones(batch_size)\n",
    "\n",
    "        # During the training of gan, the weights of discriminator should be fixed. \n",
    "        # We can enforce that by setting the trainable flag\n",
    "        discriminator.trainable=False\n",
    "\n",
    "        # Training  the GAN by alternating the training of the Discriminator and training the chained GAN model with Discriminator's weights freezed.\n",
    "        gan.train_on_batch(x, y_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_train[0:1, : , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_synth = generator.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfile.write('test.wav', samplerate, generated_synth[0].T) # samplerate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
